import metadata from './metadata.json';
import Layout from '../../../components/layout';
import {BlogImage, Meta, Caption} from '../../../components/blog';

export default Layout;

<Meta data={metadata} />

<BlogImage
  src="https://i.imgur.com/X0jy3lO.png"
  alt="Two almost identical sheep standing in a verdant field"
  caption="Similar, yet different."
/>

Today we are going to talk about Visual Snapshot Tests. We will consider why you might want visual tests, what visual testing looks like in practice, and how the Base Web team leverages visual testing in our continuous integration (CI) workflow.

## Snapshots

Before we consider _visual_ snapshot testing, let's clarify what snapshot testing is.

A snapshot test runs a process and compares the output of that process to a previous test run's output (the baseline). If the outputs match, the test passes. If the outputs do not match, the test fails and you either need to update your baseline snapshot or fix something in your source code.

Because snapshot tests are assertions of equality, snapshots need to be serializable, comparable, and deterministic. The format of the snapshot is flexible, it can be inlined into the test as a string or stored as a file— it can be a DOM tree, a JSON blob or even an image.

### Visual

This is where the _visual_ in Visual Snapshot Testing comes from. We use images as our snapshots and compare those images across test runs using an [image comparison library](https://github.com/mapbox/pixelmatch). When an image differs from our baseline image, we either update the baseline or fix the issue in source.

In Base Web, we render each of our components in a variety of states- we capture an image of each state and save all of these as baseline snapshots. Whenever we make a change to source we can run our visual snapshot tests to ensure that all of our components' visual states look as expected.

## Value Added

As we will see in a bit, introducing a suite of visual snapshots into your project comes with some maintenance burden. Taking deterministic screenshots requires running tests in a consistent environment across test runs. Test runs themselves can add significant time to your CI pipeline. Additionally, keeping hundreds or thousands of images under source control can dramatically increase the storage footprint of your repository.

If you are familiar with snapshot testing you may also wonder why we need to bother with images at all. Why not simply capture the state of the React tree as with traditional snapshot tests? Are visual snapshots worth all the overhead?

For a UI component library such as Base Web the tradeoff is certainly worth making. Here is a list of reasons why:

- **Catch visual regressions**. This is the first and most obvious reason to use visual snapshot tests. When we make a change to source code we want to be sure we have not broken any styles. For a UI library such as Base Web, styling is immensely important— its one of our main responsibilities. Having coverage over our component visuals means better quality components and better sleep at night for us maintainers.
- **Test what users see**. Traditional snapshot tests will certainly warn you when something in your code has changed, but the change detected is going to be your React implementation, not the result of that implementation which is what you actually care about.
- **Obvious updates**. It is a lot easier to decide to update a snapshot when you can see what the before and after look like. Many traditional snapshots have been erroneously updated because a developer didn't understand what had actually changed. Anyone who has worked with snapshots in the past has seen a diff with hundreds or thousands of changed lines in snapshots. An image tells the full story.
- **Centralized coverage**. Most testing strategies focus on coverage for application state and behavior. For instance, a typical UI unit test might make assertions about a rendering function's output or an end-to-end test might verify that certain elements appear on the page. But we care as much, if not more, about how our components _look_. If a color is off or something is not aligned, we want to catch that before dependent apps can consume it. By focusing thorough visual coverage on the Base Web library, downstream projects can focus on testing what actually matters to them.
- **Verifying appearances across browsers & device sizes**. One of the potential benefits of setting up visual tests is to be able to run them on different browsers and devices. Depending on your support requirements, catching an environment specific regression before it ships is a huge win.
- **A component changelog**. One of the benefits of having visual snapshots under version control is that you get a component changelog for free. By looking at the history of a snapshot you can track every visual change that's been made to the component. If your project has very thorough visual tests you can even track props and functionality through snapshot histories. The snapshots can also function as a contract to dependent projects— as a supplement to documentation, snapshots can give someone an exact picture of the state of every component.

There may be other reasons to set up visual snapshot tests, but so far these have been enough to warrant the investment for our team.

## Solutions

There are a variety of products and libraries out there which will help you with visual test coverage in your codebase. Although we ended up piecing together our own system using open source tools, it seems we should briefly touch on what other options you have before digging in yourself.

### Paid

A whole host of paid services exist to serve your visual testing needs— too many to detail here. The ones that seemed most intriguing to us were the more specialized services that focused solely on visual comparisons. Here is a list of the companies we considered (sorted alphabetically):

- [Applitools](https://applitools.com/)
- [Chromatic](https://www.chromaticqa.com/)
- [Happo](https://happo.io/)
- [Percy](https://percy.io/)
- [Screener](https://screener.io/)

With all of these paid solutions you do not have to worry about storing or comparing images yourself. The service runs a suite of comparison tests and then flags failures for you to review in their UI. Generally these runs are triggered from, and report their status to, your CI pipeline.

If you are maintaining a web based UI component library there is a decent chance you are already using Storybook in your project to develop components in isolation. Most of the services above provide libraries or integrations for using your already existing Storybook scenarios as visual snapshots.

Also of interest, most services allow you to capture images of scenarios in different environments such as IE11 or iOS Safari. This is not a trivial (or free) thing to set up for your own project- so if cross browser coverage is important for you, a paid solution may be an easy decision.

In terms of cost, all of these solutions charge per snapshot. If you have 50 scenarios and test each one in 3 environments (Chrome, IE11, iOS Safari), a single test run will generate 150 snapshots. Ultimately one of the reasons we built our own solution is that we generate over half a million snapshots a month running our visual test suite on every PR commit. This is well above the amounts quoted by these services and becomes quite expensive.

### Free

If you want to add visual tests to your codebase there are also open source solutions which you can use for free, albeit with some setup. The three below were the ones that seemed most promising for Base Web:

- [BackstopJS](https://github.com/garris/BackstopJS)
- [Loki](https://github.com/oblador/loki/)
- [Differencify](https://github.com/NimaSoroush/differencify)

All of these tools give you an API for visiting pages, taking screenshots, asserting comparisons against a baseline, and updating snapshots as needed. These tools assume that you will store your image snapshots in Git. Also of great import, these tools provide a Docker image for running the tests in a CI pipeline, where environment consistentcy is required for determinstic screenshots.

These three tools in particular were easy to get started with and provide most of the basic functionality you need to get started with visual testing. If you are considering using snapshot tests it makes sense to try one of these out first and see if it works for you.

We tried each of these solutions with Base Web, and while it was simple to get started, we ended up needing more control over the test runs. What's more the ready made containers that came with these projects were proving difficult to integrate with our existing CI pipeline— so some of the out-of-the-box utility was quickly diminished.

Eventually, we decided to roll out our own solution for visual snapshot testing. We already had a CI job set up for running end-to-end tests against our Storybook scenarios, so it seemed a simple task to extend that functionality for generating visual snapshots.

## Down to business

Let's go ahead and dive into how our solution works. After a high level overview, we can drill into some of the implementation details and cover the tradeoffs with our approach. Finally, we can go over possible improvements we might make in the future.

### Overview

1. When a pull request is opened against `origin/master`, or a commit is pushed to a branch with an open pull request, our CI pipeline (Buildkite) begins a series of parallelized jobs. One of these jobs is responsible for running our visual snapshot tests. We will call this job `VRT` (Visual Regression Tests).
2. One of the initial steps in the pipeline is spinning up a server which runs each of our Storybook scenarios on a separate page. This is used by `VRT` as well as our end-to-end test job.
3. When the Storybook scenario site is built, `VRT` runs a suite of Jest tests focused on capturing visual snapshots.
4. We use `puppeteer` (via `jest-puppeteer`) to visit each scenario page, where we execute a collection of snapshot tests. We take one snapshot of a component at a desktop size, one snapshot for mobile, one for our dark theme, and, if specified through a configuration file, we can run multiple "interactions" on a given scenario before taking snapshots.
5. We use `jest-image-snapshot` for comparing a screenshot taken with our `puppeteer` instance against our baseline image snapshot. If a snapshot does not match our baseline, we update the snapshot locally within our Docker instance.
6. After all the snapshot tests finish, `VRT` checks to see if any snapshots were updated locally. If none were updated, the job passes our `VRT` check. If there were updates, `VRT` commits those changes to a new branch, pushes the branch to GitHub, and then opens a pull request from the `VRT` branch into the original PR branch. The original branch owner is pinged for review and a comment is added to the original PR with information about the new visual snapshot PR. The `VRT` step is then failed to indicate that changes are required to the original PR branch before merging into `master`.
7. The `VRT` PR contains updated or new visual snapshot images. These can be easily reviewed in the GitHub interface. If the new snapshots look as expected, the original PR author can squash and merge the `VRT` PR into their branch. If the images look off the original author can amend their changes. Any new commit to the original PR branch will trigger a new `VRT` build, which if it passes will close any open `VRT` PR and delete the appropriate branch. If the new build does not pass, the `VRT` branch will be updated accordingly.

## Implementation

While implementing the above system, we ran into a few issues. Below we will go over some of the gotchas you might encounter while trying a custom solution.

### Flakiness

### Docker

### Puppeteer

### GitHub

## Benefits

The best part of this system is that you never have to run snapshots locally. Everything is run asynchronously in our CI pipeline. You don't want contributors having to run a Docker instance locally to generate new snapshots. It adds an especially huge hurdle for external or first time contributors. By keeping the process fully within CI, developers can focus on their actual work.

Another useful feature of this workflow is that everything is in GitHub. Developers can easily compare images within the GitHub interface, using some nifty built-in tools. Visual snapshots have a clear changelog, searchable by commit or pull request label. Also important, snapshots are public. External contributors do not have to worry about priviledges or log into a third party solution's site. In fact, developers do not need to learn anything new to benefit from visual snapshots. If they can merge a pull request, they can contribute to Base Web.

## The future

There are a few gaps in our current solution.

### Devices

The first and perhaps largest issue is that our tests only run within a Chrome browser (puppeteer). While we can modify the viewport size and do some rough simulations of a mobile device through meta tags, there is no denying it isn't the real thing. For now we have to manually investigate regressions using real devices or a service such as [BrowserStack](https://www.browserstack.com/). It would be ideal to have snapshots generated against mobile Safari and Internet Explorer 11.

### Speed

The snapshots tests add about 8 minutes to our CI pipeline. They are by far the longest running step in the process. While developers do not need to be attentive during this 8 minutes, it would be ideal to halve or quarter this length of time. Doing so would likely require us to move the tests out of Jest so we could better parallelize the job.

### Git

Our snapshots currently add about 10MB to our Git repository. It hasn't resulted in any noticeble difference for our workflow but in the future, as the project grows, we might need to make use of [Git LFS](https://git-lfs.github.com/) or move images out of Git entirely.

### A11y

Perhaps we could use the snapshot images for visual accessibilty audits. Perhaps there are other potential use-cases for the snapshot images.

### Downstream

Base Web is used by hundreds of internal projects at Uber. When we make a change to Base Web we can verify that our visual snapshots look correct - but we have no idea how our changes have affected downstream projects until we run those projects themselves. Visual testing isn't often implemented in these projects so they require manual testing.

In the future we would like to componentize our visual regression testing solution such that it could be easily added to dependent internal projects. A project might setup a few scenarios, simple static renderings of their app, and use these as as a visual contract between Base Web and that project.

### Open source

We would love to make the scripts and tools we've leveraged somehow useful for the open source community. The challenge is in bundling all of the discreete pieces. Right now we have logic spread out over GitHub, Buildkite, Docker, and custom JS scripts. There are a number of conventions and assumptions across this logic that could be easily broken if even one piece was off. In other words we have a very brittle custom solution. It works well for us but wrapping all of this logic up into a package that is more general and still useful is no small effort.

## Conclusion

The purpose of this post is to give anyone interested in Visual Regression Testing an overview of their options and some reasons why the effort might be worthwhile. There are a number of factors to consider when evaluating what will work best for your own project. By going over paid and free options, as well as the implementation and tradeoffs of our own custom solution, we hope that you might be able to save some time in your own endeavours.
