import metadata from './metadata.json';
import Layout from '../../../components/layout';
import {BlogImage, Meta, Caption} from '../../../components/blog';

export default Layout;

<Meta data={metadata} />

<BlogImage
  src="https://i.imgur.com/X0jy3lO.png"
  alt="Two almost identical sheep standing in a verdant field"
  caption="Similar, yet different."
/>

Today we are going to talk about Visual Snapshot Tests. We will consider why you might want visual tests, what visual testing looks like in practice, and how the Base Web team leverages visual testing in our continuous integration (CI) workflow.

## Snapshots

Before we consider _visual_ snapshot testing, let's clarify what snapshot testing is.

A snapshot test runs a process and compares the output of that process to a previous test run's output (the baseline). If the outputs match, the test passes. If the outputs do not match, the test fails and you either need to update your baseline snapshot or fix something in your source code.

Because snapshot tests are assertions of equality, snapshots need to be serializable, comparable, and deterministic. The format of the snapshot is flexible, it can be inlined into the test as a string or stored as a file— it can be a DOM tree, a JSON blob or even an image.

### Visual

This is where the _visual_ in Visual Snapshot Testing comes from. We use images as our snapshots and compare those images across test runs using an [image comparison library](https://github.com/mapbox/pixelmatch). When an image differs from our baseline image, we either update the baseline or fix the issue in source.

In Base Web, we render each of our components in a variety of states- we capture an image of each state and save all of these as baseline snapshots. Whenever we make a change to source we can run our visual snapshot tests to ensure that all of our components' visual states look as expected.

## Value Added

As we will see in a bit, introducing a suite of visual snapshots into your project comes with some maintenance burden. Taking deterministic screenshots requires running tests in a consistent environment across test runs. Test runs themselves can add significant time to your CI pipeline. Additionally, keeping hundreds or thousands of images under source control can dramatically increase the storage footprint of your repository.

If you are familiar with snapshot testing you may also wonder why we need to bother with images at all. Why not simply capture the state of the React tree as with traditional snapshot tests? Are visual snapshots worth all the overhead?

For a UI component library such as Base Web the tradeoff is certainly worth making. Here is a list of reasons why:

- **Catch visual regressions**. This is the first and most obvious reason to use visual snapshot tests. When we make a change to source code we want to be sure we have not broken any styles. For a UI library such as Base Web, styling is immensely important— its one of our main responsibilities. Having coverage over our component visuals means better quality components and better sleep at night for us maintainers.
- **Test what users see**. Traditional snapshot tests will certainly warn you when something in your code has changed, but the change detected is going to be your React implementation, not the result of that implementation which is what you actually care about.
- **Obvious updates**. It is a lot easier to decide to update a snapshot when you can see what the before and after look like. Many traditional snapshots have been erroneously updated because a developer didn't understand what had actually changed. Anyone who has worked with snapshots in the past has seen a diff with hundreds or thousands of changed lines in snapshots. An image tells the full story.
- **Centralized coverage**. Most testing strategies focus on coverage for application state and behavior. For instance, a typical UI unit test might make assertions about a rendering function's output or an end-to-end test might verify that certain elements appear on the page. But we care as much, if not more, about how our components _look_. If a color is off or something is not aligned, we want to catch that before dependent apps can consume it. By focusing thorough visual coverage on the Base Web library, downstream projects can focus on testing what actually matters to them.
- **Verifying appearances across browsers & device sizes**. One of the potential benefits of setting up visual tests is to be able to run them on different browsers and devices. Depending on your support requirements, catching an environment specific regression before it ships is a huge win.
- **A component changelog**. One of the benefits of having visual snapshots under version control is that you get a component changelog for free. By looking at the history of a snapshot you can track every visual change that's been made to the component. If your project has very thorough visual tests you can even track props and functionality through snapshot histories. The snapshots can also function as a contract to dependent projects— as a supplement to documentation, snapshots can give someone an exact picture of the state of every component.

There may be other reasons to set up visual snapshot tests, but so far these have been enough to warrant the investment for our team.

## Solutions

There are a variety of products and libraries out there which will help you with visual test coverage in your codebase. Although we ended up piecing together our own system using open source tools, it seems we should briefly touch on what other options you have before digging in yourself.

### Paid

A whole host of paid services exist to serve your visual testing needs— too many to detail here. The ones that seemed most intriguing to us were the more specialized services that focused solely on visual comparisons. Here is a list of the companies we considered (sorted alphabetically):

- [Applitools](https://applitools.com/)
- [Chromatic](https://www.chromaticqa.com/)
- [Happo](https://happo.io/)
- [Percy](https://percy.io/)
- [Screener](https://screener.io/)

With all of these paid solutions you do not have to worry about storing or comparing images yourself. The service runs a suite of comparison tests and then flags failures for you to review in their UI. Generally these runs are triggered from, and report their status to, your CI pipeline.

If you are maintaining a web based UI component library there is a decent chance you are already using Storybook in your project to develop components in isolation. Most of the services above provide libraries or integrations for using your already existing Storybook scenarios as visual snapshots.

Also of interest, most services allow you to capture images of scenarios in different environments such as IE11 or iOS Safari. This is not a trivial (or free) thing to set up for your own project- so if cross browser coverage is important for you, a paid solution may be an easy decision.

In terms of cost, all of these solutions charge per snapshot. If you have 50 scenarios and test each one in 3 environments (Chrome, IE11, iOS Safari), a single test run will generate 150 snapshots. Ultimately one of the reasons we built our own solution is that we generate over half a million snapshots a month running our visual test suite on every PR commit. This is well above the amounts quoted by these services and becomes quite expensive.

### Free

If you want to add visual tests to your codebase there are also open source solutions which you can use for free, albeit with some setup. The three below were the ones that seemed most promising for Base Web:

- [BackstopJS](https://github.com/garris/BackstopJS)
- [Loki](https://github.com/oblador/loki/)
- [Differencify](https://github.com/NimaSoroush/differencify)

All of these tools give you an API for visiting pages, taking screenshots, asserting comparisons against a baseline, and updating snapshots as needed. These tools assume that you will store your image snapshots in Git. Also of great import, these tools provide a Docker image for running the tests in a CI pipeline, where environment consistentcy is required for determinstic screenshots.

These three tools in particular were easy to get started with and provide most of the basic functionality you need to get started with visual testing. If you are considering using snapshot tests it makes sense to try one of these out first and see if it works for you.

We tried each of these solutions with Base Web, and while it was simple to get started, we ended up needing more control over the test runs. What's more the ready made containers that came with these projects were proving difficult to integrate with our existing CI pipeline— so some of the out-of-the-box utility was quickly diminished.

Eventually, we decided to roll out our own solution for visual snapshot testing. We already had a CI job set up for running end-to-end tests against our Storybook scenarios, so it seemed a simple task to extend that functionality for generating visual snapshots.

## Down to business

Let's go ahead and dive into how our solution works. After a high level overview, we can drill into some of the implementation details, cover the tradeoffs with our approach, and finally we can go over possible improvements we might make in the future.

### Overview

1. When a pull request is opened against `origin/master`, or a commit is pushed to a branch with an open pull request, our CI pipeline (Buildkite) begins a series of parallelized jobs. One of these jobs is responsible for running our visual snapshot tests. We will call this job `VRT` (Visual Regression Tests).
2. One of the initial steps in the pipeline is spinning up a server which runs each of our Storybook scenarios on a separate page. This is used by `VRT` as well as our end-to-end test job.
3. When the Storybook scenario site is built, `VRT` runs a suite of Jest tests focused on capturing visual snapshots.
4. We use `puppeteer` (via `jest-puppeteer`) to visit each scenario page, where we execute a collection of snapshot tests. We take one snapshot of a component at a desktop size, one snapshot for mobile, one for our dark theme, and, if specified through a configuration file, we can run multiple "interactions" on a given scenario before taking snapshots.
5. We use `jest-image-snapshot` for comparing a screenshot taken with our `puppeteer` instance against our baseline image snapshot. If a snapshot does not match our baseline, we update the snapshot locally within our Docker instance.
6. After all the snapshot tests finish, `VRT` checks to see if any snapshots were updated locally. If none were updated, the job passes our `VRT` check. If there were updates, `VRT` commits those changes to a new branch, pushes the branch to GitHub, and then opens a pull request from the `VRT` branch into the original PR branch. The original branch owner is pinged for review and a comment is added to the original PR with information about the new visual snapshot PR. The `VRT` step is then failed to indicate that changes are required to the original PR branch before merging into `master`.
7. The `VRT` PR contains updated or new visual snapshot images. These can be easily reviewed in the GitHub interface. If the new snapshots look as expected, the original PR author can squash and merge the `VRT` PR into their branch. If the images look off the original author can amend their changes. Any new commit to the original PR branch will trigger a new `VRT` build, which if it passes will close any open `VRT` PR and delete the appropriate branch. If the new build does not pass, the `VRT` branch will be updated accordingly.

## Implementation

There are a lot of different pieces needed to get the above system to work. Below, we will go over some of the tools you will need and some of the gotchas you might encounter while building out a custom visual testing solution.

### Puppeteer

[Puppeteer](https://pptr.dev/) is a library for automating usage of Chrome via the Chrome Devtools Protocol. Puppeteer is what allows us to easily generate screenshots for our snapshot tests. If using Jest, you can use [`jest-puppeteer`](https://github.com/smooth-code/jest-puppeteer) to set up a global reference to the Puppeteer Page (`page`) object in your unit tests. In many of the code snippets below you will see references to this global `page` object.

### A static site

At some point you will need to host a site somewhere which Puppeteer can visit in order to render your UI. As part of our CI pipeline, we build a small static site that renders ever Storybook scenario in our codebase at a unique URL. Once this site is built, we run visual regression tests as well as end-to-end tests against it.

### Extending Jest

We already had Jest, Puppeteer, and [`jest-puppeteer`](https://github.com/smooth-code/jest-puppeteer) set up for our end-to-end tests so it was fairly easy to use the same tools to run our visual regression tests. We required only one extra library: [`jest-image-snapshot`](https://github.com/americanexpress/jest-image-snapshot). This library provides a new Jest assertion to make comparing visual snapshots as easy as writing a unit test:

```js
const image = await page.screenshot(); // generate a new image with puppeteer
expect(image).toMatchImageSnapshot(); // compare to baseline image
```

If you already use snapshots in your codebase there is almost nothing new to learn here. The only difference from normal inline snapshots is that you will have image files added to your project which will need to be kept under version control as baselines for future tests.

For a JS/React project this is a very low overhead way to add visual tests to your codebase.

### The test run

All of our snapshot tests are run under one Jest `describe` block. It essentially looks like this:

```js
describe('Visual Regression Tests', () => {
  getAllScenarios().forEach(scenario => {
    describe(scenario, () => {
      it('desktop', async () => {
        await setupDesktop();
        await snapshot();
      });
      it('mobile', async () => {
        await setupMobile();
        await snapshot();
      });
      it('dark', async () => {
        await setupDark();
        await snapshot();
      });
      getAllInteractionsForScenario(scenario).forEach(interaction => {
        it(interaction.name, async () => {
          await interaction.behavior();
          await snapshot();
        });
      });
    });
  });
});
```

This is a fairly abbreviated version of the real code, but it shows the structure of our test run. One of the key bits here is the `getAllScenarios()` function - this will return every Storybook scenario file in the codebase. The result is that you do not have to add any configuration to add a test- **if there is a Storybook scenario it will automatically become a visual regression test**.

That said, there _is_ a configuration file for when you need to modify the behavior of a specific snapshot test. This configuration object is a little clunky but it lets you target a specific scenario and do things like skip the scenario or add additional interaction snapshots based on that scenario.

The config object looks something like this:

```js
const config = {
  'input-password': {
    interactions: [
      {
        name: 'togglesMask',
        behavior: async page => {
          const toggleSelector = `[data-e2e="mask-toggle"]`;
          await page.$(toggleSelector);
          await page.click(toggleSelector);
        },
      },
    ],
  },
};
```

Each `behavior` for an interaction is a function that takes a [Puppeteer Page](https://pptr.dev/#?product=Puppeteer&version=v2.0.0&show=api-class-page) instance and returns a Promise that when resolved has arranged the UI into a desired state for a snapshot.

In the above example, we will generate a new snapshot based on the `input-password` scenario. The behavior function will run before a snapshot is saved as `input-password__togglesMask-snap.png`.

### Screenshot dimensions

We went through a few iterations of tinkering with screenshot dimensions. At first we only took a screenshot of the root element on the page. This had the advantage of capturing as small as possible an area- leading to smaller images. We had to change the approach however due to a few edge cases:

For one, any component that uses CSS to absolutely position elements, such as a modal or dropdown, will no longer contribute to the size of the root element. This means that most of the relevant UI will be left out of the final screenshot. This could be solved with some configuration parameters, but we wanted things to work well by default.

Another issue was that some elements would expand past the desired viewport width. Becuase of this we weren't getting a real picture of what a mobile user would see and screenshots varied in width from snapshot to snapshot. The solution was to "clamp" the screenshot to a fixed width while capturing the maximum height of the page.

With Puppeteer that looks something like this:

```js
// Use Chrome Devtools Protocol to get the scroll height of the page.
const client = await page.target().createCDPSession();
const metrics = await client.send('Page.getLayoutMetrics');
const height = Math.ceil(metrics.contentSize.height);
const image = await page.screenshot({
  clip: {
    x: 0,
    y: 0,
    width: VIEWPORT_WIDTH[viewport],
    height: height,
  },
});
```

The result is that every screenshot has the same dimensions for the given viewport (mobile/desktop) across test runs. This makes comparisons for developers a little bit easier, but it also shows us a more realistic simulation of what an end user might see on their device.

The added size to each screenshot is actually fairly limited. The white/black areas can be compressed fairly effeciently so most of the empty space does not contribute to the size of the file. Even though the overall dimensions of the images increased by about 40% the storage footprint only increased around 10%.

### Docker

One of the first things you will encounter when adding visual tests to your codebase is that the enviroment of a test run matters. Fonts and colors are not rendered in exactly the same way across all operating systems. The discrepency is enough that a pixel diffing algorithm will consider images generated in each operating system as incompatible, even if they have rendered the exact same UI. The result is that if someone runs the test suite on Mac and checks in the generated snapshots, all of the tests will fail when run on Windows or Linux.

The solution for this is to always run the tests in the same environment, which for most projects means running tests in a Docker container. Our CI pipeline was already configured to use Docker & Linux so this became our "environment of truth". Note that it is also possible to set up your CI pipeline to always run tests on a Mac, Windows, or other environment.

### Flakiness

Snapshot tests should be deterministic. That is, they should yield the same result if run in the same environment with the same inputs. Yet even after nailing down a consistent environment there can be variations in the UI that lead to flakey test results. When your test runs take 5-10 minutes, you don't want to have flakey tests be the reason your tests did not pass.

Here are the two main things that produced flakiness in our snapshots:

#### **Animations**

If you have any components that make use of transitions or animations you are going to want to shut those down. You don't want a millisecond difference in rendering time to affect the outcome of the test. Thankfully, it should be as simple as adding a little CSS to your page:

```css
*,
*::before,
*::after {
  -moz-transition: none !important;
  transition: none !important;
  -moz-animation: none !important;
  animation: none !important;
  caret-color: transparent !important;
}
```

#### **Asynchronicity**

When testing components that require interactions, you often need to wait for the UI to change before you can proceed to the next step in the interaction. It might be tempting to simply pad steps with arbirary wait times, but trust me, you do not want do this.

```js
await page.waitFor(250); // waits for 250ms before proceeding
```

This should be a last resort as it will almost certainly flake at some point. The preferable approach is to wait for assertions on the page to pass. Take this example from one of our interaction snapshots:

```js
'select-search-single': {
    interactions: [
      {
        name: 'open',
        behavior: async page => {
          const inputSelector = `[data-baseweb="select"]`;
          const dropdownSelector = `[role="listbox"]`;
          await page.waitForSelector(inputSelector);
          await page.click(inputSelector);
          await page.waitForSelector(dropdownSelector);
        },
      },
    ],
  },
```

`page`, an instance of [Page](https://pptr.dev/#?product=Puppeteer&version=v2.0.0&show=api-class-page), has numerous methods for awaiting events or assertions. While it is still possible for an assertion to time out, it is far, far less likely to occur.

### GitHub

## Benefits

The best part of this system is that you never have to run snapshots locally. Everything is run asynchronously in our CI pipeline. You don't want contributors having to run a Docker instance locally to generate new snapshots. It adds an especially huge hurdle for external or first time contributors. By keeping the process fully within CI, developers can focus on their actual work.

Another useful feature of this workflow is that everything is in GitHub. Developers can easily compare images within the GitHub interface, using some nifty built-in tools. Visual snapshots have a clear changelog, searchable by commit or pull request label. Also important, snapshots are public. External contributors do not have to worry about priviledges or log into a third party solution's site. In fact, developers do not need to learn anything new to benefit from visual snapshots. If they can merge a pull request, they can contribute to Base Web.

## The future

There are a few gaps in our current solution.

### Devices

The first and perhaps largest issue is that our tests only run within a Chrome browser (puppeteer). While we can modify the viewport size and do some rough simulations of a mobile device through meta tags, there is no denying it isn't the real thing. For now we have to manually investigate regressions using real devices or a service such as [BrowserStack](https://www.browserstack.com/). It would be ideal to have snapshots generated against mobile Safari and Internet Explorer 11.

### Speed

The snapshots tests add about 8 minutes to our CI pipeline. They are by far the longest running step in the process. While developers do not need to be attentive during this 8 minutes, it would be ideal to halve or quarter this length of time. Doing so would likely require us to move the tests out of Jest so we could better parallelize the job.

### Git

Our snapshots currently add about 10MB to our Git repository. It hasn't resulted in any noticeble difference for our workflow but in the future, as the project grows, we might need to make use of [Git LFS](https://git-lfs.github.com/) or move images out of Git entirely.

### A11y

Perhaps we could use the snapshot images for visual accessibilty audits. Perhaps there are other potential use-cases for the snapshot images.

### Downstream

Base Web is used by hundreds of internal projects at Uber. When we make a change to Base Web we can verify that our visual snapshots look correct - but we have no idea how our changes have affected downstream projects until we run those projects themselves. Visual testing isn't often implemented in these projects so they require manual testing.

In the future we would like to componentize our visual regression testing solution such that it could be easily added to dependent internal projects. A project might setup a few scenarios, simple static renderings of their app, and use these as as a visual contract between Base Web and that project.

### Open source

We would love to make the scripts and tools we've leveraged somehow useful for the open source community. The challenge is in bundling all of the discreete pieces. Right now we have logic spread out over GitHub, Buildkite, Docker, and custom JS scripts. There are a number of conventions and assumptions across this logic that could be easily broken if even one piece was off. In other words we have a very brittle custom solution. It works well for us but wrapping all of this logic up into a package that is more general and still useful is no small effort.

## Conclusion

The purpose of this post is to give anyone interested in Visual Regression Testing an overview of their options and some reasons why the effort might be worthwhile. There are a number of factors to consider when evaluating what will work best for your own project. By going over paid and free options, as well as the implementation and tradeoffs of our own custom solution, we hope that you might be able to save some time in your own endeavours.
